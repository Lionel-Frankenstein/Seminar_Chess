{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e80d9b",
   "metadata": {},
   "source": [
    "1. Learning rate finder\n",
    "2. Train each Optimizer\n",
    "3. Drop the last optimzier for the next architecture\n",
    "4. Inrease the number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe0ec9",
   "metadata": {},
   "source": [
    "To implement\n",
    "1. Dropout\n",
    "2. Weight Decay\n",
    "3. LR scheduler\n",
    "4. Train with one csv and test on another csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de681450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f08e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvs = []\n",
    "for csv in os.listdir(\"./lichess/data_1\"):\n",
    "    if \"csv\" in csv:\n",
    "        csvs.append(csv)\n",
    "len(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef539610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, batch_size):\n",
    "    targets_numpy = df.result.values\n",
    "    features_numpy = df.loc[:,df.columns != \"result\"].values\n",
    "\n",
    "    features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "        features_numpy,\n",
    "        targets_numpy,\n",
    "        test_size = 0.2,\n",
    "        random_state = random.randint(0,100)\n",
    "    )\n",
    "\n",
    "    featuresTrain = torch.from_numpy(features_train).type(torch.float32)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.float32)\n",
    "\n",
    "    featuresTest = torch.from_numpy(features_test).type(torch.float32)\n",
    "    targetsTest = torch.from_numpy(targets_test).type(torch.float32)\n",
    "\n",
    "    # Pytorch train and test sets\n",
    "    train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "    test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "    # data loader\n",
    "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(model, optimizer, train_loader, lr_range, steps):\n",
    "    loss_list = []    \n",
    "    error = nn.L1Loss()\n",
    "\n",
    "    for lr in tqdm(torch.logspace(lr_range[0], lr_range[1], steps=steps)):\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        temp_loss_list = []\n",
    "        \n",
    "        for boards, labels in train_loader:\n",
    "            train = boards.view(-1, 1088)\n",
    "            labels = labels.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train)\n",
    "            loss = error(outputs, labels)\n",
    "            temp_loss_list.append(loss.detach().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        temp_loss = np.mean(temp_loss_list)\n",
    "        loss_list.append(temp_loss)\n",
    "    \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29683883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_range = [-6, 0]\n",
    "lr_list = torch.logspace(lr_range[0], lr_range[1], steps=100)\n",
    "lr_list = [tensor.detach().item() for tensor in lr_list]\n",
    "lr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7579adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_loader, test_loader, model, optimizer, stop):\n",
    "#     error = nn.L1Loss()\n",
    "#     train_loss_list = []\n",
    "#     val_loss_list = []\n",
    "#     min_loss = float('inf')\n",
    "#     epoch_count = 1\n",
    "#     stop_count = 0\n",
    "    \n",
    "#     while True:\n",
    "#         temp_loss_list = []\n",
    "        \n",
    "#         for boards, labels in train_loader:\n",
    "#             train = boards.view(-1, 1088)\n",
    "#             labels = labels.view(-1, 1)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(train)\n",
    "#             loss = error(outputs, labels)\n",
    "#             temp_loss_list.append(loss.detach().item())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         train_loss = np.mean(temp_loss_list)\n",
    "#         train_loss_list.append(train_loss)\n",
    "#         temp_loss_list = []\n",
    "\n",
    "#         for boards, labels in test_loader:\n",
    "#             test = boards.view(-1, 1088)\n",
    "#             outputs = model(test)\n",
    "#             temp_loss_list.append(error(outputs, labels.view(-1, 1)).detach().item())\n",
    "        \n",
    "#         val_loss = np.mean(temp_loss_list)\n",
    "#         val_loss_list.append(val_loss)\n",
    "        \n",
    "#         if val_loss < min_loss:\n",
    "#             min_loss = val_loss\n",
    "#             stop_count = 0\n",
    "#         else:\n",
    "#             stop_count += 1\n",
    "\n",
    "#         print(f\"EPOCH: {epoch_count}, TRAIN_LOSS: {round(float(train_loss), 4)}, VAL:_LOSS: {round(float(val_loss), 4)}, STOP_COUNT: {stop_count}\")\n",
    "#         epoch_count += 1\n",
    "\n",
    "#         if stop_count >= stop:\n",
    "#             print(f\"MIN_VAL_LOSS: {min(val_loss_list)}\")\n",
    "#             break\n",
    "    \n",
    "#     return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f93ed",
   "metadata": {},
   "source": [
    "# Hidden Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, output_dim):\n",
    "        super(ANNModel1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab16393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN1 lr finder\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = random.randint(0,55)\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adadelta\": torch.optim.Adadelta,\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"ASGD\": torch.optim.ASGD,\n",
    "    \"NAdam\": torch.optim.NAdam,\n",
    "    \"RAdam\": torch.optim.RAdam,\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel1(input_dim, hidden_dim_0, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6fb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdccd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adadelta\": (torch.optim.Adadelta, 0.1),\n",
    "    \"Adagrad\": (torch.optim.Adagrad, 0.01),\n",
    "    \"Adam\": (torch.optim.Adam, 1e-4),\n",
    "    \"Adamax\": (torch.optim.Adamax, 1e-4),\n",
    "    \"ASGD\": (torch.optim.ASGD, 0.1),\n",
    "    \"NAdam\": (torch.optim.NAdam, 1e-4),\n",
    "    \"RAdam\": (torch.optim.RAdam, 1e-4),\n",
    "    \"RMSprop\": (torch.optim.RMSprop, 1e-4),\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel1(input_dim, hidden_dim_0, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, 10)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b24cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27afe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebdb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca2e19",
   "metadata": {},
   "source": [
    "# Hidden Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, output_dim):\n",
    "        super(ANNModel2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f765e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN2 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adadelta\": torch.optim.Adadelta,\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"NAdam\": torch.optim.NAdam,\n",
    "    \"RAdam\": torch.optim.RAdam,\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel2(input_dim, hidden_dim_0, hidden_dim_1, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befefa62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adadelta\": (torch.optim.Adadelta, 0.1),\n",
    "    \"Adagrad\": (torch.optim.Adagrad, 0.001),\n",
    "    \"Adam\": (torch.optim.Adam, 1e-5),\n",
    "    \"Adamax\": (torch.optim.Adamax, 1e-4),\n",
    "    \"NAdam\": (torch.optim.NAdam, 1e-5),\n",
    "    \"RAdam\": (torch.optim.RAdam, 1e-5),\n",
    "    \"RMSprop\": (torch.optim.RMSprop, 1e-5),\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel2(input_dim, hidden_dim_0, hidden_dim_1, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, 10)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caa3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37063197",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54d821",
   "metadata": {},
   "source": [
    "# Hidden Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(ANNModel3, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN3 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"NAdam\": torch.optim.NAdam,\n",
    "    \"RAdam\": torch.optim.RAdam,\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel3(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca82245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_3.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4adea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f3973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ANN3 train\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adagrad\": (torch.optim.Adagrad, 0.001),\n",
    "    \"Adam\": (torch.optim.Adam, 1e-5),\n",
    "    \"Adamax\": (torch.optim.Adamax, 5e-5),\n",
    "    \"NAdam\": (torch.optim.NAdam, 1e-5),\n",
    "    \"RAdam\": (torch.optim.RAdam, 1e-5),\n",
    "    \"RMSprop\": (torch.optim.RMSprop, 1e-5),\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel3(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, 10)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d153ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78eab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_3.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64939a71",
   "metadata": {},
   "source": [
    "# Hidden Layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04556673",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel4(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, output_dim):\n",
    "        super(ANNModel4, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN4 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "hidden_dim_3 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adagrad\": torch.optim.Adagrad,\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"NAdam\": torch.optim.NAdam,\n",
    "    \"RAdam\": torch.optim.RAdam,\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel4(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_4.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fef1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ANN4 train\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "hidden_dim_3 = 512\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adagrad\": (torch.optim.Adagrad, 0.001),\n",
    "    \"Adamax\": (torch.optim.Adamax, 5e-5),\n",
    "    \"NAdam\": (torch.optim.NAdam, 1e-5),\n",
    "    \"RAdam\": (torch.optim.RAdam, 1e-5),\n",
    "    \"RMSprop\": (torch.optim.RMSprop, 1e-5),\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel4(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, 10)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb5e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_4.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491aff82",
   "metadata": {},
   "source": [
    "# Hidden Layer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel5(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim):\n",
    "        super(ANNModel5, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.fc6 = nn.Linear(hd_4, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24515b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN5 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"NAdam\": torch.optim.NAdam,\n",
    "    \"RAdam\": torch.optim.RAdam,\n",
    "    \"RMSprop\": torch.optim.RMSprop,\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel5(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_5.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN5 train\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adagrad\": (torch.optim.Adagrad, 0.001),\n",
    "    \"Adamax\": (torch.optim.Adamax, 5e-5),\n",
    "    \"NAdam\": (torch.optim.NAdam, 1e-5),\n",
    "    \"RAdam\": (torch.optim.RAdam, 1e-5),\n",
    "    \"RMSprop\": (torch.optim.RMSprop, 1e-5),\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel5(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, 10)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_5.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c872af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_5.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf03fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = [np.mean(df[col].dropna()[-10:]) for col in df.columns]\n",
    "loss_dict = dict(zip(df.columns, loss_list))\n",
    "loss_dict = sorted(loss_dict.items(), key=lambda x: x[1])\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6cf48",
   "metadata": {},
   "source": [
    "# Hidden Layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel6(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim):\n",
    "        super(ANNModel6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)        \n",
    "        self.fc7 = nn.Linear(hd_5, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc7(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08185f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN6 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel6(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, train_loader, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45431d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_6.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_6.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN6 train\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "stop = 10\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.3),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.03),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel6(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd051bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_6.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_6.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[keys[0]], marker='o', label = 'SGD')\n",
    "plt.plot(df[keys[1]], marker='o', label = 'SGD-Momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df2dc0",
   "metadata": {},
   "source": [
    "# Hidden Layer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel7(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim):\n",
    "        super(ANNModel7, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc8(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ead53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN7 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel7(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, train_loader, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_7.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_7.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ac84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN7 train\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "stop = 10\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.5),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.05),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel7(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_7.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_7.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[keys[0]], marker='o', label = 'SGD')\n",
    "plt.plot(df[keys[1]], marker='o', label = 'SGD-Momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087b70d",
   "metadata": {},
   "source": [
    "# Hidden Layer 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim):\n",
    "        super(ANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "output_dim = 1\n",
    "batch_size = 128\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    # \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, train_loader, (-2, 2), 50)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74420e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_8.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_8.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 train\n",
    "train_index = 32\n",
    "batch_size = 128\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "stop = 10\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 2),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.1),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa059aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_8.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_8.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[keys[0]], marker='o', label = 'SGD')\n",
    "plt.plot(df[keys[1]], marker='o', label = 'SGD-Momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5128d56",
   "metadata": {},
   "source": [
    "# Adding dropout, batch normalization, and weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213219a",
   "metadata": {},
   "source": [
    "Adding batch normalization in addition to dropout since the model seems to be unable to converge with adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob):\n",
    "        super(NewANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.bn1 = nn.BatchNorm1d(hd_0) \n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hd_1)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.bn3 = nn.BatchNorm1d(hd_2)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)          \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.bn4 = nn.BatchNorm1d(hd_3)\n",
    "        self.dropout4 = nn.Dropout(p=drop_prob)           \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.bn5 = nn.BatchNorm1d(hd_4)\n",
    "        self.dropout5 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.bn6 = nn.BatchNorm1d(hd_5)\n",
    "        self.dropout6 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)\n",
    "        self.bn7 = nn.BatchNorm1d(hd_6)\n",
    "        self.dropout7 = nn.Dropout(p=drop_prob)     \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.bn8 = nn.BatchNorm1d(hd_7)\n",
    "        self.dropout8 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)     \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)      \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)  \n",
    "        x = torch.relu(self.bn4(self.fc4(x)))    \n",
    "        x = self.dropout4(x)  \n",
    "        x = torch.relu(self.bn5(self.fc5(x)))     \n",
    "        x = self.dropout5(x)  \n",
    "        x = torch.relu(self.bn6(self.fc6(x)))\n",
    "        x = self.dropout6(x)  \n",
    "        x = torch.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.dropout7(x)  \n",
    "        x = torch.relu(self.bn8(self.fc8(x)))\n",
    "        x = self.dropout8(x)  \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95624095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 lr finder\n",
    "# Drop ASGD\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "output_dim = 1\n",
    "drop_prob = 0.2\n",
    "batch_size = 128\n",
    "weight_decay = 0.001\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = NewANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, weight_decay=weight_decay)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, train_loader, (-4, 0), 50)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "lr_range = [-4, 0]\n",
    "lr_list = torch.logspace(lr_range[0], lr_range[1], steps=50)\n",
    "lr_list = [tensor.detach().item() for tensor in lr_list]\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_8_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_8_new.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewANN8 train\n",
    "train_index = 32\n",
    "batch_size = 128\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "drop_prob = 0.2\n",
    "weight_decay = 0.001\n",
    "stop = 20\n",
    "output_dim = 1\n",
    "\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.02),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.002),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = NewANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b17b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_8_new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_8_new.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbe3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[keys[0]], marker='o', label = 'SGD')\n",
    "plt.plot(df[keys[1]], marker='o', label = 'SGD-Momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ec39e",
   "metadata": {},
   "source": [
    "# Experiment with 64 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb22dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ANN64(nn.Module):\n",
    "    def __init__(self, input_size=1088, output_size=1, hidden_size=512, dropout_prob=0.2):\n",
    "        super(ANN64, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(63):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_prob))\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33936a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN64 lr finder\n",
    "batch_size = 128\n",
    "weight_decay = 0.001\n",
    "\n",
    "train_index = 32\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, _ = prepare_data(train_df, batch_size)\n",
    "\n",
    "loss_dict = {}\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"SGD-Momentum\": torch.optim.SGD,\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_class in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    model = ANN64()\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-6, weight_decay=weight_decay)\n",
    "    \n",
    "    loss_list = lr_finder(model, optimizer, train_loader, (-6, 0), 100)\n",
    "    loss_dict[algo_name] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loss_dict)\n",
    "lr_range = [-6, 0]\n",
    "lr_list = torch.logspace(lr_range[0], lr_range[1], steps=100)\n",
    "lr_list = [tensor.detach().item() for tensor in lr_list]\n",
    "df['LR'] = lr_list\n",
    "df.to_csv('./model_histories/ANN/lr_finder_hd_64.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./model_histories/ANN/lr_finder_hd_64.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(keys[idx])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df['LR'], df[keys[idx]], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed92f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 1e-3),\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 1e-4),\n",
    "}\n",
    "\n",
    "train_index = 32\n",
    "batch_size = 128\n",
    "print(f\"TRAIN_INDEX: {train_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, test_loader = prepare_data(train_df, batch_size)\n",
    "stop = 10\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANN64()\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[algo_name] = train_loss_list\n",
    "    val_loss_dict[algo_name] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efe349",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/structure_selector_val_loss_hd_64.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e89a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/structure_selector_val_loss_hd_64.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(df.columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[keys[0]], marker='o', label = 'SGD')\n",
    "plt.plot(df[keys[1]], marker='o', label = 'SGD-Momentum')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b191268",
   "metadata": {},
   "source": [
    "# Training with one csv and testing with another\n",
    "This didn't work. Probably simultaneously traning with one csv and testing with another is too difficult for the model to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = random.randint(0,55)\n",
    "while True:\n",
    "    test_index = random.randint(0,55)\n",
    "    if train_index != test_index:\n",
    "        break\n",
    "batch_size = 128\n",
    "print(f\"TRAIN_INDEX: {train_index}, TEST_INDEX: {test_index}\")\n",
    "train_df = pd.read_csv(\"./lichess/data_1/\" + csvs[train_index])\n",
    "train_loader, _ = prepare_data(train_df, batch_size)\n",
    "test_df = pd.read_csv(\"./lichess/data_1/\" + csvs[test_index])\n",
    "_, test_loader = prepare_data(test_df, batch_size)\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, output_dim):\n",
    "        super(ANNModel1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d379712",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict = {\n",
    "    \"Adamax\": (torch.optim.Adamax, 1e-4),\n",
    "}\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel1(input_dim, hd_0, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_1_adamax\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_1_adamax\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, output_dim):\n",
    "        super(ANNModel2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel2(input_dim, hidden_dim_0, hidden_dim_1, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_2_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_2_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(ANNModel3, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN3 train\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel3(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_3_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_3_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ca68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel4(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, output_dim):\n",
    "        super(ANNModel4, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN4 train\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "hidden_dim_3 = 512\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel4(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_4_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_4_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6630fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel5(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim):\n",
    "        super(ANNModel5, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.fc6 = nn.Linear(hd_4, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ca0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN5 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel5(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_5_SGD\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_5_SGD\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel6(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim):\n",
    "        super(ANNModel6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)        \n",
    "        self.fc7 = nn.Linear(hd_5, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc7(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN6 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "stop = 20\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.03),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel6(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_6_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_6_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel7(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim):\n",
    "        super(ANNModel7, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc8(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96000dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN7 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "stop = 20\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.05),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel7(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_7_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_7_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad483a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim):\n",
    "        super(ANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "stop = 20\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.1),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"ann_8_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"ann_8_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob):\n",
    "        super(NewANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.bn1 = nn.BatchNorm1d(hd_0) \n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hd_1)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.bn3 = nn.BatchNorm1d(hd_2)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)          \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.bn4 = nn.BatchNorm1d(hd_3)\n",
    "        self.dropout4 = nn.Dropout(p=drop_prob)           \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.bn5 = nn.BatchNorm1d(hd_4)\n",
    "        self.dropout5 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.bn6 = nn.BatchNorm1d(hd_5)\n",
    "        self.dropout6 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)\n",
    "        self.bn7 = nn.BatchNorm1d(hd_6)\n",
    "        self.dropout7 = nn.Dropout(p=drop_prob)     \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.bn8 = nn.BatchNorm1d(hd_7)\n",
    "        self.dropout8 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)     \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)      \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)  \n",
    "        x = torch.relu(self.bn4(self.fc4(x)))    \n",
    "        x = self.dropout4(x)  \n",
    "        x = torch.relu(self.bn5(self.fc5(x)))     \n",
    "        x = self.dropout5(x)  \n",
    "        x = torch.relu(self.bn6(self.fc6(x)))\n",
    "        x = self.dropout6(x)  \n",
    "        x = torch.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.dropout7(x)  \n",
    "        x = torch.relu(self.bn8(self.fc8(x)))\n",
    "        x = self.dropout8(x)  \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "drop_prob = 0.2\n",
    "stop = 20\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.02),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = NewANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list, val_loss_list = train(train_loader, test_loader, model, optimizer, stop)\n",
    "    train_loss_dict[\"new_ann_8_SGD-Momentum\"] = train_loss_list\n",
    "    val_loss_dict[\"new_ann_8_SGD-Momentum\"] = val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e9c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ranking_list = [np.mean(val_loss_dict[x][-10:]) for x in val_loss_dict]\n",
    "val_loss_ranking_dict = dict(zip(list(val_loss_dict.keys()), val_loss_ranking_list))\n",
    "val_loss_ranking_dict = sorted(val_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "val_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/best_models_val_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ranking_list = [np.mean(train_loss_dict[x][-10:]) for x in train_loss_dict]\n",
    "train_loss_ranking_dict = dict(zip(list(train_loss_dict.keys()), train_loss_ranking_list))\n",
    "train_loss_ranking_dict = sorted(train_loss_ranking_dict.items(), key=lambda x: x[1])\n",
    "train_loss_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57157e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in train_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in train_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/best_models_train_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59270fb6",
   "metadata": {},
   "source": [
    "# Training with multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f36c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    error = nn.L1Loss()\n",
    "    temp_loss_list = []\n",
    "    for boards, labels in test_loader:\n",
    "        test = boards.view(-1, 1088)\n",
    "        outputs = model(test)\n",
    "        temp_loss_list.append(error(outputs, labels.view(-1, 1)).detach().item())\n",
    "    val_loss = np.mean(temp_loss_list)\n",
    "    print(f\"VAL:_LOSS: {round(float(val_loss), 4)}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c192b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, test_laoder, model, optimizer, stop):\n",
    "    error = nn.L1Loss()\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    test_loss_list = []\n",
    "    min_loss = float('inf')\n",
    "    epoch_count = 1\n",
    "    stop_count = 0\n",
    "    \n",
    "    while True:\n",
    "        temp_loss_list = []\n",
    "        \n",
    "        for boards, labels in train_loader:\n",
    "            train = boards.view(-1, 1088)\n",
    "            labels = labels.view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train)\n",
    "            loss = error(outputs, labels)\n",
    "            temp_loss_list.append(loss.detach().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = np.mean(temp_loss_list)\n",
    "        train_loss_list.append(train_loss)\n",
    "        temp_loss_list = []\n",
    "\n",
    "        for boards, labels in valid_loader:\n",
    "            val = boards.view(-1, 1088)\n",
    "            outputs = model(val)\n",
    "            temp_loss_list.append(error(outputs, labels.view(-1, 1)).detach().item())\n",
    "        \n",
    "        val_loss = np.mean(temp_loss_list)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            stop_count = 0\n",
    "        else:\n",
    "            stop_count += 1\n",
    "\n",
    "        if test_loader:\n",
    "            test_loss = test(model, test_loader)\n",
    "            test_loss_list.append(test_loss)\n",
    "\n",
    "        print(f\"EPOCH: {epoch_count}, TRAIN_LOSS: {round(float(train_loss), 4)}, VAL:_LOSS: {round(float(val_loss), 4)}, TEST_LOSS: {round(float(test_loss), 4)} STOP_COUNT: {stop_count}\")\n",
    "        epoch_count += 1\n",
    "\n",
    "        if stop_count >= stop:\n",
    "            print(f\"MIN_VAL_LOSS: {min(val_loss_list)}\")\n",
    "            break\n",
    "    return train_loss_list, val_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c4535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(model, optimizer, test_loader, n_files, stop):\n",
    "    total_train_loss_list = []\n",
    "    total_val_loss_list = []\n",
    "    total_test_loss_list = []\n",
    "    for i in range(n_files):\n",
    "        print(f\"FILE_NUMBER: {i}\")\n",
    "        df = pd.read_csv(\"./lichess/data_1/\" + csvs[i])\n",
    "        train_loader, val_loader = prepare_data(df, batch_size)\n",
    "        train_loss_list, val_loss_list, test_loss_list = train(train_loader, val_loader, test_loader, model, optimizer, stop)\n",
    "        total_train_loss_list.append(train_loss_list)\n",
    "        total_val_loss_list.append(val_loss_list)\n",
    "        total_test_loss_list.append(test_loss_list)\n",
    "    return total_train_loss_list, total_val_loss_list, total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58c2b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_INDEX: 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_files = 1\n",
    "test_index = n_files\n",
    "stop = 10\n",
    "print(f\"TEST_INDEX: {test_index}\")\n",
    "test_df = pd.read_csv(\"./lichess/data_1/\" + csvs[test_index])\n",
    "_, test_loader = prepare_data(test_df, batch_size)\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "test_loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ea82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, output_dim):\n",
    "        super(ANNModel1, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)  \n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b8a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamax\n",
      "TRAINING\n",
      "FILE_NUMBER: 0\n",
      "VAL:_LOSS: 0.2269\n",
      "EPOCH: 1, TRAIN_LOSS: 0.2246, VAL:_LOSS: 0.2176, TEST_LOSS: 0.2269 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2245\n",
      "EPOCH: 2, TRAIN_LOSS: 0.2134, VAL:_LOSS: 0.209, TEST_LOSS: 0.2245 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2234\n",
      "EPOCH: 3, TRAIN_LOSS: 0.2059, VAL:_LOSS: 0.2033, TEST_LOSS: 0.2234 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2217\n",
      "EPOCH: 4, TRAIN_LOSS: 0.2, VAL:_LOSS: 0.1975, TEST_LOSS: 0.2217 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2201\n",
      "EPOCH: 5, TRAIN_LOSS: 0.195, VAL:_LOSS: 0.1932, TEST_LOSS: 0.2201 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2192\n",
      "EPOCH: 6, TRAIN_LOSS: 0.1906, VAL:_LOSS: 0.1895, TEST_LOSS: 0.2192 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2186\n",
      "EPOCH: 7, TRAIN_LOSS: 0.1867, VAL:_LOSS: 0.1858, TEST_LOSS: 0.2186 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2181\n",
      "EPOCH: 8, TRAIN_LOSS: 0.1828, VAL:_LOSS: 0.1835, TEST_LOSS: 0.2181 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2173\n",
      "EPOCH: 9, TRAIN_LOSS: 0.1792, VAL:_LOSS: 0.1793, TEST_LOSS: 0.2173 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2161\n",
      "EPOCH: 10, TRAIN_LOSS: 0.1754, VAL:_LOSS: 0.1761, TEST_LOSS: 0.2161 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2171\n",
      "EPOCH: 11, TRAIN_LOSS: 0.1717, VAL:_LOSS: 0.1735, TEST_LOSS: 0.2171 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2157\n",
      "EPOCH: 12, TRAIN_LOSS: 0.168, VAL:_LOSS: 0.1696, TEST_LOSS: 0.2157 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.216\n",
      "EPOCH: 13, TRAIN_LOSS: 0.1642, VAL:_LOSS: 0.1661, TEST_LOSS: 0.216 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2152\n",
      "EPOCH: 14, TRAIN_LOSS: 0.1601, VAL:_LOSS: 0.1621, TEST_LOSS: 0.2152 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2145\n",
      "EPOCH: 15, TRAIN_LOSS: 0.1563, VAL:_LOSS: 0.1584, TEST_LOSS: 0.2145 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2148\n",
      "EPOCH: 16, TRAIN_LOSS: 0.1522, VAL:_LOSS: 0.1551, TEST_LOSS: 0.2148 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.215\n",
      "EPOCH: 17, TRAIN_LOSS: 0.1482, VAL:_LOSS: 0.1513, TEST_LOSS: 0.215 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2144\n",
      "EPOCH: 18, TRAIN_LOSS: 0.1441, VAL:_LOSS: 0.1475, TEST_LOSS: 0.2144 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2144\n",
      "EPOCH: 19, TRAIN_LOSS: 0.1403, VAL:_LOSS: 0.1444, TEST_LOSS: 0.2144 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2142\n",
      "EPOCH: 20, TRAIN_LOSS: 0.1364, VAL:_LOSS: 0.1407, TEST_LOSS: 0.2142 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2153\n",
      "EPOCH: 21, TRAIN_LOSS: 0.1326, VAL:_LOSS: 0.1383, TEST_LOSS: 0.2153 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2152\n",
      "EPOCH: 22, TRAIN_LOSS: 0.1288, VAL:_LOSS: 0.134, TEST_LOSS: 0.2152 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2154\n",
      "EPOCH: 23, TRAIN_LOSS: 0.1251, VAL:_LOSS: 0.1315, TEST_LOSS: 0.2154 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2161\n",
      "EPOCH: 24, TRAIN_LOSS: 0.1214, VAL:_LOSS: 0.1278, TEST_LOSS: 0.2161 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2176\n",
      "EPOCH: 25, TRAIN_LOSS: 0.1181, VAL:_LOSS: 0.1273, TEST_LOSS: 0.2176 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2171\n",
      "EPOCH: 26, TRAIN_LOSS: 0.1148, VAL:_LOSS: 0.1223, TEST_LOSS: 0.2171 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2164\n",
      "EPOCH: 27, TRAIN_LOSS: 0.1114, VAL:_LOSS: 0.1188, TEST_LOSS: 0.2164 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2172\n",
      "EPOCH: 28, TRAIN_LOSS: 0.108, VAL:_LOSS: 0.1165, TEST_LOSS: 0.2172 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.218\n",
      "EPOCH: 29, TRAIN_LOSS: 0.1052, VAL:_LOSS: 0.1131, TEST_LOSS: 0.218 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2178\n",
      "EPOCH: 30, TRAIN_LOSS: 0.1021, VAL:_LOSS: 0.1101, TEST_LOSS: 0.2178 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2182\n",
      "EPOCH: 31, TRAIN_LOSS: 0.0994, VAL:_LOSS: 0.1076, TEST_LOSS: 0.2182 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.22\n",
      "EPOCH: 32, TRAIN_LOSS: 0.0966, VAL:_LOSS: 0.1055, TEST_LOSS: 0.22 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2203\n",
      "EPOCH: 33, TRAIN_LOSS: 0.0939, VAL:_LOSS: 0.1029, TEST_LOSS: 0.2203 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.22\n",
      "EPOCH: 34, TRAIN_LOSS: 0.0916, VAL:_LOSS: 0.1004, TEST_LOSS: 0.22 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2217\n",
      "EPOCH: 35, TRAIN_LOSS: 0.089, VAL:_LOSS: 0.0986, TEST_LOSS: 0.2217 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2223\n",
      "EPOCH: 36, TRAIN_LOSS: 0.0868, VAL:_LOSS: 0.0969, TEST_LOSS: 0.2223 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2225\n",
      "EPOCH: 37, TRAIN_LOSS: 0.0846, VAL:_LOSS: 0.0949, TEST_LOSS: 0.2225 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2237\n",
      "EPOCH: 38, TRAIN_LOSS: 0.0824, VAL:_LOSS: 0.0931, TEST_LOSS: 0.2237 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2249\n",
      "EPOCH: 39, TRAIN_LOSS: 0.0803, VAL:_LOSS: 0.0911, TEST_LOSS: 0.2249 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2243\n",
      "EPOCH: 40, TRAIN_LOSS: 0.0784, VAL:_LOSS: 0.0893, TEST_LOSS: 0.2243 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2249\n",
      "EPOCH: 41, TRAIN_LOSS: 0.0763, VAL:_LOSS: 0.0873, TEST_LOSS: 0.2249 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2269\n",
      "EPOCH: 42, TRAIN_LOSS: 0.0744, VAL:_LOSS: 0.0875, TEST_LOSS: 0.2269 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2259\n",
      "EPOCH: 43, TRAIN_LOSS: 0.073, VAL:_LOSS: 0.0844, TEST_LOSS: 0.2259 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2268\n",
      "EPOCH: 44, TRAIN_LOSS: 0.0712, VAL:_LOSS: 0.0825, TEST_LOSS: 0.2268 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2281\n",
      "EPOCH: 45, TRAIN_LOSS: 0.0694, VAL:_LOSS: 0.0807, TEST_LOSS: 0.2281 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2292\n",
      "EPOCH: 46, TRAIN_LOSS: 0.0678, VAL:_LOSS: 0.0801, TEST_LOSS: 0.2292 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2294\n",
      "EPOCH: 47, TRAIN_LOSS: 0.0666, VAL:_LOSS: 0.0782, TEST_LOSS: 0.2294 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2296\n",
      "EPOCH: 48, TRAIN_LOSS: 0.0649, VAL:_LOSS: 0.0768, TEST_LOSS: 0.2296 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2304\n",
      "EPOCH: 49, TRAIN_LOSS: 0.0633, VAL:_LOSS: 0.0754, TEST_LOSS: 0.2304 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2335\n",
      "EPOCH: 50, TRAIN_LOSS: 0.062, VAL:_LOSS: 0.078, TEST_LOSS: 0.2335 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2318\n",
      "EPOCH: 51, TRAIN_LOSS: 0.0605, VAL:_LOSS: 0.0734, TEST_LOSS: 0.2318 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2318\n",
      "EPOCH: 52, TRAIN_LOSS: 0.0594, VAL:_LOSS: 0.0715, TEST_LOSS: 0.2318 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.233\n",
      "EPOCH: 53, TRAIN_LOSS: 0.058, VAL:_LOSS: 0.0707, TEST_LOSS: 0.233 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2333\n",
      "EPOCH: 54, TRAIN_LOSS: 0.0568, VAL:_LOSS: 0.0709, TEST_LOSS: 0.2333 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2335\n",
      "EPOCH: 55, TRAIN_LOSS: 0.0558, VAL:_LOSS: 0.0685, TEST_LOSS: 0.2335 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2356\n",
      "EPOCH: 56, TRAIN_LOSS: 0.0545, VAL:_LOSS: 0.0694, TEST_LOSS: 0.2356 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2358\n",
      "EPOCH: 57, TRAIN_LOSS: 0.0536, VAL:_LOSS: 0.0671, TEST_LOSS: 0.2358 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2354\n",
      "EPOCH: 58, TRAIN_LOSS: 0.0523, VAL:_LOSS: 0.0653, TEST_LOSS: 0.2354 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2355\n",
      "EPOCH: 59, TRAIN_LOSS: 0.0513, VAL:_LOSS: 0.0639, TEST_LOSS: 0.2355 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2359\n",
      "EPOCH: 60, TRAIN_LOSS: 0.0504, VAL:_LOSS: 0.063, TEST_LOSS: 0.2359 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2371\n",
      "EPOCH: 61, TRAIN_LOSS: 0.0495, VAL:_LOSS: 0.0629, TEST_LOSS: 0.2371 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2375\n",
      "EPOCH: 62, TRAIN_LOSS: 0.0484, VAL:_LOSS: 0.0622, TEST_LOSS: 0.2375 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2385\n",
      "EPOCH: 63, TRAIN_LOSS: 0.0472, VAL:_LOSS: 0.0605, TEST_LOSS: 0.2385 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2386\n",
      "EPOCH: 64, TRAIN_LOSS: 0.0465, VAL:_LOSS: 0.06, TEST_LOSS: 0.2386 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2387\n",
      "EPOCH: 65, TRAIN_LOSS: 0.0456, VAL:_LOSS: 0.059, TEST_LOSS: 0.2387 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2404\n",
      "EPOCH: 66, TRAIN_LOSS: 0.0449, VAL:_LOSS: 0.0584, TEST_LOSS: 0.2404 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2402\n",
      "EPOCH: 67, TRAIN_LOSS: 0.0443, VAL:_LOSS: 0.0573, TEST_LOSS: 0.2402 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2405\n",
      "EPOCH: 68, TRAIN_LOSS: 0.0433, VAL:_LOSS: 0.0569, TEST_LOSS: 0.2405 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2416\n",
      "EPOCH: 69, TRAIN_LOSS: 0.0425, VAL:_LOSS: 0.0568, TEST_LOSS: 0.2416 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2418\n",
      "EPOCH: 70, TRAIN_LOSS: 0.0418, VAL:_LOSS: 0.0556, TEST_LOSS: 0.2418 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.242\n",
      "EPOCH: 71, TRAIN_LOSS: 0.041, VAL:_LOSS: 0.055, TEST_LOSS: 0.242 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2426\n",
      "EPOCH: 72, TRAIN_LOSS: 0.0403, VAL:_LOSS: 0.0538, TEST_LOSS: 0.2426 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2436\n",
      "EPOCH: 73, TRAIN_LOSS: 0.0396, VAL:_LOSS: 0.0529, TEST_LOSS: 0.2436 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2444\n",
      "EPOCH: 74, TRAIN_LOSS: 0.0391, VAL:_LOSS: 0.0543, TEST_LOSS: 0.2444 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2456\n",
      "EPOCH: 75, TRAIN_LOSS: 0.0383, VAL:_LOSS: 0.0538, TEST_LOSS: 0.2456 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.245\n",
      "EPOCH: 76, TRAIN_LOSS: 0.0376, VAL:_LOSS: 0.0511, TEST_LOSS: 0.245 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2468\n",
      "EPOCH: 77, TRAIN_LOSS: 0.037, VAL:_LOSS: 0.0524, TEST_LOSS: 0.2468 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2458\n",
      "EPOCH: 78, TRAIN_LOSS: 0.0364, VAL:_LOSS: 0.0503, TEST_LOSS: 0.2458 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2467\n",
      "EPOCH: 79, TRAIN_LOSS: 0.036, VAL:_LOSS: 0.0506, TEST_LOSS: 0.2467 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2469\n",
      "EPOCH: 80, TRAIN_LOSS: 0.0351, VAL:_LOSS: 0.0502, TEST_LOSS: 0.2469 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2465\n",
      "EPOCH: 81, TRAIN_LOSS: 0.0347, VAL:_LOSS: 0.0491, TEST_LOSS: 0.2465 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2469\n",
      "EPOCH: 82, TRAIN_LOSS: 0.0342, VAL:_LOSS: 0.0478, TEST_LOSS: 0.2469 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2481\n",
      "EPOCH: 83, TRAIN_LOSS: 0.0335, VAL:_LOSS: 0.0482, TEST_LOSS: 0.2481 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2476\n",
      "EPOCH: 84, TRAIN_LOSS: 0.0333, VAL:_LOSS: 0.0469, TEST_LOSS: 0.2476 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2505\n",
      "EPOCH: 85, TRAIN_LOSS: 0.0326, VAL:_LOSS: 0.0487, TEST_LOSS: 0.2505 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2531\n",
      "EPOCH: 86, TRAIN_LOSS: 0.0322, VAL:_LOSS: 0.0516, TEST_LOSS: 0.2531 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2501\n",
      "EPOCH: 87, TRAIN_LOSS: 0.0317, VAL:_LOSS: 0.0461, TEST_LOSS: 0.2501 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2495\n",
      "EPOCH: 88, TRAIN_LOSS: 0.031, VAL:_LOSS: 0.045, TEST_LOSS: 0.2495 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2501\n",
      "EPOCH: 89, TRAIN_LOSS: 0.0307, VAL:_LOSS: 0.0444, TEST_LOSS: 0.2501 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2513\n",
      "EPOCH: 90, TRAIN_LOSS: 0.0303, VAL:_LOSS: 0.0442, TEST_LOSS: 0.2513 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2509\n",
      "EPOCH: 91, TRAIN_LOSS: 0.0298, VAL:_LOSS: 0.0438, TEST_LOSS: 0.2509 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2514\n",
      "EPOCH: 92, TRAIN_LOSS: 0.0296, VAL:_LOSS: 0.0432, TEST_LOSS: 0.2514 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2521\n",
      "EPOCH: 93, TRAIN_LOSS: 0.029, VAL:_LOSS: 0.043, TEST_LOSS: 0.2521 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2524\n",
      "EPOCH: 94, TRAIN_LOSS: 0.0285, VAL:_LOSS: 0.0432, TEST_LOSS: 0.2524 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2535\n",
      "EPOCH: 95, TRAIN_LOSS: 0.0281, VAL:_LOSS: 0.0432, TEST_LOSS: 0.2535 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2531\n",
      "EPOCH: 96, TRAIN_LOSS: 0.0278, VAL:_LOSS: 0.0417, TEST_LOSS: 0.2531 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2535\n",
      "EPOCH: 97, TRAIN_LOSS: 0.0272, VAL:_LOSS: 0.0411, TEST_LOSS: 0.2535 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.254\n",
      "EPOCH: 98, TRAIN_LOSS: 0.0271, VAL:_LOSS: 0.0414, TEST_LOSS: 0.254 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2544\n",
      "EPOCH: 99, TRAIN_LOSS: 0.0267, VAL:_LOSS: 0.0415, TEST_LOSS: 0.2544 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2558\n",
      "EPOCH: 100, TRAIN_LOSS: 0.0261, VAL:_LOSS: 0.0418, TEST_LOSS: 0.2558 STOP_COUNT: 3\n",
      "VAL:_LOSS: 0.2552\n",
      "EPOCH: 101, TRAIN_LOSS: 0.0258, VAL:_LOSS: 0.0402, TEST_LOSS: 0.2552 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2549\n",
      "EPOCH: 102, TRAIN_LOSS: 0.0255, VAL:_LOSS: 0.0395, TEST_LOSS: 0.2549 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2565\n",
      "EPOCH: 103, TRAIN_LOSS: 0.0251, VAL:_LOSS: 0.0406, TEST_LOSS: 0.2565 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2562\n",
      "EPOCH: 104, TRAIN_LOSS: 0.0246, VAL:_LOSS: 0.0399, TEST_LOSS: 0.2562 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2563\n",
      "EPOCH: 105, TRAIN_LOSS: 0.0245, VAL:_LOSS: 0.0387, TEST_LOSS: 0.2563 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2561\n",
      "EPOCH: 106, TRAIN_LOSS: 0.0242, VAL:_LOSS: 0.0381, TEST_LOSS: 0.2561 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2565\n",
      "EPOCH: 107, TRAIN_LOSS: 0.0238, VAL:_LOSS: 0.0381, TEST_LOSS: 0.2565 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2573\n",
      "EPOCH: 108, TRAIN_LOSS: 0.0236, VAL:_LOSS: 0.0382, TEST_LOSS: 0.2573 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2578\n",
      "EPOCH: 109, TRAIN_LOSS: 0.0233, VAL:_LOSS: 0.0381, TEST_LOSS: 0.2578 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2576\n",
      "EPOCH: 110, TRAIN_LOSS: 0.0228, VAL:_LOSS: 0.0371, TEST_LOSS: 0.2576 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2601\n",
      "EPOCH: 111, TRAIN_LOSS: 0.0225, VAL:_LOSS: 0.0383, TEST_LOSS: 0.2601 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.2593\n",
      "EPOCH: 112, TRAIN_LOSS: 0.0224, VAL:_LOSS: 0.0371, TEST_LOSS: 0.2593 STOP_COUNT: 2\n",
      "VAL:_LOSS: 0.2587\n",
      "EPOCH: 113, TRAIN_LOSS: 0.0221, VAL:_LOSS: 0.0361, TEST_LOSS: 0.2587 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2591\n",
      "EPOCH: 114, TRAIN_LOSS: 0.0217, VAL:_LOSS: 0.0358, TEST_LOSS: 0.2591 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2595\n",
      "EPOCH: 115, TRAIN_LOSS: 0.0215, VAL:_LOSS: 0.0359, TEST_LOSS: 0.2595 STOP_COUNT: 1\n",
      "VAL:_LOSS: 0.26\n",
      "EPOCH: 116, TRAIN_LOSS: 0.0211, VAL:_LOSS: 0.0356, TEST_LOSS: 0.26 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2606\n",
      "EPOCH: 117, TRAIN_LOSS: 0.0211, VAL:_LOSS: 0.0354, TEST_LOSS: 0.2606 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2609\n",
      "EPOCH: 118, TRAIN_LOSS: 0.0208, VAL:_LOSS: 0.0353, TEST_LOSS: 0.2609 STOP_COUNT: 0\n",
      "VAL:_LOSS: 0.2611\n",
      "EPOCH: 119, TRAIN_LOSS: 0.0203, VAL:_LOSS: 0.0359, TEST_LOSS: 0.2611 STOP_COUNT: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 151\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m optimizer_class(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTRAINING\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m total_train_loss_list, total_val_loss_list, total_test_loss_list \u001b[39m=\u001b[39m train_all(model, optimizer, test_loader, n_files, stop)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m train_loss_dict[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mann_hd_1_file_\u001b[39m\u001b[39m{\u001b[39;00mn_files\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m total_train_loss_list\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m val_loss_dict[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mann_hd_1_file_\u001b[39m\u001b[39m{\u001b[39;00mn_files\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m total_val_loss_list\n",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 151\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./lichess/data_1/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m csvs[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_loader, val_loader \u001b[39m=\u001b[39m prepare_data(df, batch_size)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_loss_list, val_loss_list, test_loss_list \u001b[39m=\u001b[39m train(train_loader, val_loader, test_loader, model, optimizer, stop)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_train_loss_list\u001b[39m.\u001b[39mappend(train_loss_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m total_val_loss_list\u001b[39m.\u001b[39mappend(val_loss_list)\n",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 151\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m boards, labels \u001b[39min\u001b[39;00m valid_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     val \u001b[39m=\u001b[39m boards\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1088\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     temp_loss_list\u001b[39m.\u001b[39mappend(error(outputs, labels\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m val_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(temp_loss_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 151\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y303sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)  \n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_dict = {\n",
    "    \"Adamax\": (torch.optim.Adamax, 1e-4),\n",
    "}\n",
    "\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "output_dim = 1\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    print(algo_name)\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel1(input_dim, hd_0, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_1_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_1_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_1_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, output_dim):\n",
    "        super(ANNModel2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62785788",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel2(input_dim, hidden_dim_0, hidden_dim_1, output_dim)\n",
    "    initial_params = model.state_dict().copy()\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_2_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_2_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_2_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim):\n",
    "        super(ANNModel3, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim_0, hidden_dim_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN3 train\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel3(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_3_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_3_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_3_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel4(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, output_dim):\n",
    "        super(ANNModel4, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN4 train\n",
    "input_dim = 1088\n",
    "hidden_dim_0 = 512\n",
    "hidden_dim_1 = 512\n",
    "hidden_dim_2 = 512\n",
    "hidden_dim_3 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.01),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel4(input_dim, hidden_dim_0, hidden_dim_1, hidden_dim_2, hidden_dim_3, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_4_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_4_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_4_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ba5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel5(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim):\n",
    "        super(ANNModel5, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.fc6 = nn.Linear(hd_4, output_dim)\n",
    "\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.relu5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "\n",
    "        x = self.sig1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN5 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.1),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel5(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_5_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_5_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_5_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel6(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim):\n",
    "        super(ANNModel6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)        \n",
    "        self.fc7 = nn.Linear(hd_5, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc7(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN6 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.03),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel6(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_6_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_6_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_6_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel7(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim):\n",
    "        super(ANNModel7, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc8(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e63330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN7 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.05),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel7(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_7_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_7_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_7_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc244d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim):\n",
    "        super(ANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)        \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)        \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)        \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)        \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)         \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      \n",
    "        x = torch.relu(self.fc2(x))      \n",
    "        x = torch.relu(self.fc3(x))      \n",
    "        x = torch.relu(self.fc4(x))      \n",
    "        x = torch.relu(self.fc5(x))      \n",
    "        x = torch.relu(self.fc6(x)) \n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x)) \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f945ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN8 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "output_dim = 1\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD-Momentum\": (torch.optim.SGD, 0.1),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_8_file_{n_files}'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_8_file_{n_files}'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_8_file_{n_files}'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691de7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewANNModel8(nn.Module):\n",
    "    def __init__(self, input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob):\n",
    "        super(NewANNModel8, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hd_0)\n",
    "        self.bn1 = nn.BatchNorm1d(hd_0) \n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc2 = nn.Linear(hd_0, hd_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hd_1)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)        \n",
    "        self.fc3 = nn.Linear(hd_1, hd_2)\n",
    "        self.bn3 = nn.BatchNorm1d(hd_2)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)          \n",
    "        self.fc4 = nn.Linear(hd_2, hd_3)\n",
    "        self.bn4 = nn.BatchNorm1d(hd_3)\n",
    "        self.dropout4 = nn.Dropout(p=drop_prob)           \n",
    "        self.fc5 = nn.Linear(hd_3, hd_4)\n",
    "        self.bn5 = nn.BatchNorm1d(hd_4)\n",
    "        self.dropout5 = nn.Dropout(p=drop_prob)      \n",
    "        self.fc6 = nn.Linear(hd_4, hd_5)\n",
    "        self.bn6 = nn.BatchNorm1d(hd_5)\n",
    "        self.dropout6 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc7 = nn.Linear(hd_5, hd_6)\n",
    "        self.bn7 = nn.BatchNorm1d(hd_6)\n",
    "        self.dropout7 = nn.Dropout(p=drop_prob)     \n",
    "        self.fc8 = nn.Linear(hd_6, hd_7)\n",
    "        self.bn8 = nn.BatchNorm1d(hd_7)\n",
    "        self.dropout8 = nn.Dropout(p=drop_prob)   \n",
    "        self.fc9 = nn.Linear(hd_7, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)     \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)      \n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)  \n",
    "        x = torch.relu(self.bn4(self.fc4(x)))    \n",
    "        x = self.dropout4(x)  \n",
    "        x = torch.relu(self.bn5(self.fc5(x)))     \n",
    "        x = self.dropout5(x)  \n",
    "        x = torch.relu(self.bn6(self.fc6(x)))\n",
    "        x = self.dropout6(x)  \n",
    "        x = torch.relu(self.bn7(self.fc7(x)))\n",
    "        x = self.dropout7(x)  \n",
    "        x = torch.relu(self.bn8(self.fc8(x)))\n",
    "        x = self.dropout8(x)  \n",
    "        \n",
    "        x = torch.sigmoid(self.fc9(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f25e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_INDEX: 2\n",
      "TRAINING\n",
      "FILE_NUMBER: 0\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 167\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m optimizer_class(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTRAINING\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m train_all(model, optimizer, n_files, stop)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTESTING\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m val_loss \u001b[39m=\u001b[39m test(model, test_loader)\n",
      "\u001b[1;32m/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb Cell 167\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_files):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFILE_NUMBER: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./lichess/data_1/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m csvs[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_loader, test_loader \u001b[39m=\u001b[39m prepare_data(df, batch_size)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/horatio/Desktop/Seminar/Chess/ann_main_v1.ipynb#Y312sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loss_list, val_loss_list \u001b[39m=\u001b[39m train(train_loader, test_loader, model, optimizer, stop)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# NewANN8 train\n",
    "input_dim = 1088\n",
    "hd_0 = 512\n",
    "hd_1 = 512\n",
    "hd_2 = 512\n",
    "hd_3 = 512\n",
    "hd_4 = 512\n",
    "hd_5 = 512\n",
    "hd_6 = 512\n",
    "hd_7 = 512\n",
    "drop_prob = 0.2\n",
    "weight_decay = 0.001\n",
    "output_dim = 1\n",
    "stop = 20\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"SGD\": (torch.optim.SGD, 0.02),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = NewANNModel8(input_dim, hd_0, hd_1, hd_2, hd_3, hd_4, hd_5, hd_6, hd_7, output_dim, drop_prob)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    print(\"TRAINING\")\n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_8_file_{n_files}_r'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_8_file_{n_files}_r'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_8_file_{n_files}_r'] = total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43111c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in train_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in train_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/best_models_train_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/best_models_train_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede568f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in val_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in val_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/best_models_val_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/best_models_val_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(lst) for lst in test_loss_dict.values())\n",
    "loss_filled_results = {k: v + [None] * (max_len - len(v)) for k, v in test_loss_dict.items()}\n",
    "df = pd.DataFrame(loss_filled_results)\n",
    "df.to_csv(\"./model_histories/ANN/best_models_test_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./model_histories/ANN/best_models_test_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681727d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2564729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import chess\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6a1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, batch_size):\n",
    "    targets_numpy = df.result.values\n",
    "    features_numpy = df.loc[:,df.columns != \"result\"].values\n",
    "\n",
    "    features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "        features_numpy,\n",
    "        targets_numpy,\n",
    "        test_size = 0.2,\n",
    "        random_state = random.randint(0,100)\n",
    "    )\n",
    "\n",
    "    featuresTrain = torch.from_numpy(features_train).type(torch.float32)\n",
    "    targetsTrain = torch.from_numpy(targets_train).type(torch.float32)\n",
    "\n",
    "    featuresTest = torch.from_numpy(features_test).type(torch.float32)\n",
    "    targetsTest = torch.from_numpy(targets_test).type(torch.float32)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "    test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(test, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2337a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_INDEX: 27\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "n_files = 27\n",
    "test_index = n_files\n",
    "stop = 5\n",
    "print(f\"TEST_INDEX: {test_index}\")\n",
    "test_df = pd.read_csv(f\"./gm/data_1/bitboard_{test_index}.csv\")\n",
    "_, test_loader = prepare_data(test_df, batch_size)\n",
    "train_loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "test_loss_dict = {}\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f98e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    error = nn.MSELoss()\n",
    "    temp_loss_list = []\n",
    "    for boards, labels in test_loader:\n",
    "        val = boards.view(-1, 1088).to(device)\n",
    "        labels = labels.view(-1, 1).to(device)\n",
    "        outputs = model(val)\n",
    "        temp_loss_list.append(error(outputs, labels).detach().item())\n",
    "    val_loss = np.mean(temp_loss_list)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc24b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, test_laoder, model, optimizer, stop):\n",
    "    error = nn.MSELoss()\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    test_loss_list = []\n",
    "    min_loss = float('inf')\n",
    "    epoch_count = 1\n",
    "    stop_count = 0\n",
    "    \n",
    "    while True:\n",
    "        temp_loss_list = []\n",
    "        \n",
    "        for boards, labels in train_loader:\n",
    "            train = boards.view(-1, 1088).to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train)\n",
    "            loss = error(outputs, labels)\n",
    "            temp_loss_list.append(loss.detach().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss = np.mean(temp_loss_list)\n",
    "        train_loss_list.append(train_loss)\n",
    "        temp_loss_list = []\n",
    "\n",
    "        for boards, labels in valid_loader:\n",
    "            val = boards.view(-1, 1088).to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "            outputs = model(val)\n",
    "            temp_loss_list.append(error(outputs, labels).detach().item())\n",
    "        \n",
    "        val_loss = np.mean(temp_loss_list)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            stop_count = 0\n",
    "        else:\n",
    "            stop_count += 1\n",
    "\n",
    "        if test_loader:\n",
    "            test_loss = test(model, test_loader)\n",
    "            test_loss_list.append(test_loss)\n",
    "\n",
    "        print(f\"EPOCH: {epoch_count}, TRAIN_LOSS: {round(float(train_loss), 4)}, VAL_LOSS: {round(float(val_loss), 4)}, TEST_LOSS: {round(float(test_loss), 4)} STOP_COUNT: {stop_count}\")\n",
    "        epoch_count += 1\n",
    "\n",
    "        if stop_count > stop:\n",
    "            print(f\"AVG_TRAIN_LOSS: {np.mean(train_loss_list[stop * (-1):])}\")\n",
    "            print(f\"AVG_VAL_LOSS: {np.mean(val_loss_list[stop * (-1):])}\")\n",
    "            print(f\"AVG_TEST_LOSS: {np.mean(test_loss_list[stop * (-1):])}\")\n",
    "            break\n",
    "    return train_loss_list, val_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870714aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(model, optimizer, test_loader, n_files, stop):\n",
    "    total_train_loss_list = []\n",
    "    total_val_loss_list = []\n",
    "    total_test_loss_list = []\n",
    "    file_order = [i for i in range(n_files)]\n",
    "    random.shuffle(file_order)\n",
    "    for i, file in enumerate(file_order):\n",
    "        print(f\"{i+1}TH FILE, FILE_ID: {file}\")\n",
    "        df = pd.read_csv(f\"./gm/data_1/bitboard_{file}.csv\")\n",
    "        train_loader, val_loader = prepare_data(df, batch_size)\n",
    "        train_loss_list, val_loss_list, test_loss_list = train(train_loader, val_loader, test_loader, model, optimizer, stop)\n",
    "        total_train_loss_list.append(train_loss_list)\n",
    "        total_val_loss_list.append(val_loss_list)\n",
    "        total_test_loss_list.append(test_loss_list)\n",
    "    return total_train_loss_list, total_val_loss_list, total_test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5a10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN16(nn.Module):\n",
    "    def __init__(self, input_size=1088, output_size=1, hidden_size=512, dropout_prob=0.2):\n",
    "        super(ANN16, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout_prob))\n",
    "        for _ in range(15):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_prob))\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = torch.tanh(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb0b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1TH FILE, FILE_ID: 22\n",
      "EPOCH: 1, TRAIN_LOSS: 0.6406, VAL_LOSS: 0.6262, TEST_LOSS: 0.6374 STOP_COUNT: 0\n",
      "EPOCH: 2, TRAIN_LOSS: 0.6167, VAL_LOSS: 0.6156, TEST_LOSS: 0.6272 STOP_COUNT: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m---> 17\u001b[0m total_train_loss_list, total_val_loss_list, total_test_loss_list \u001b[38;5;241m=\u001b[39m train_all(model, optimizer, test_loader, n_files, stop)\n\u001b[1;32m     18\u001b[0m train_loss_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mann_hd_64_file_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m total_train_loss_list\n\u001b[1;32m     19\u001b[0m val_loss_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mann_hd_64_file_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m total_val_loss_list\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mtrain_all\u001b[0;34m(model, optimizer, test_loader, n_files, stop)\u001b[0m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gm/data_1/bitboard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m prepare_data(df, batch_size)\n\u001b[0;32m---> 11\u001b[0m train_loss_list, val_loss_list, test_loss_list \u001b[38;5;241m=\u001b[39m train(train_loader, val_loader, test_loader, model, optimizer, stop)\n\u001b[1;32m     12\u001b[0m total_train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss_list)\n\u001b[1;32m     13\u001b[0m total_val_loss_list\u001b[38;5;241m.\u001b[39mappend(val_loss_list)\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, test_laoder, model, optimizer, stop)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m error(outputs, labels)\n\u001b[1;32m     19\u001b[0m     temp_loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(temp_loss_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/seminar_chess/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weight_decay = 0.001\n",
    "\n",
    "optimizer_dict = {\n",
    "    \"Adam\": (torch.optim.Adam, 0.00012618),\n",
    "}\n",
    "\n",
    "for algo_name, optimizer_tuple in optimizer_dict.items():\n",
    "    optimizer_class = optimizer_tuple[0]\n",
    "    learning_rate = optimizer_tuple[1]\n",
    "    model = ANN16()\n",
    "    model.to(device)\n",
    "    if algo_name == \"SGD-Momentum\":\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    total_train_loss_list, total_val_loss_list, total_test_loss_list = train_all(model, optimizer, test_loader, n_files, stop)\n",
    "    train_loss_dict[f'ann_hd_64_file_{n_files}_r'] = total_train_loss_list\n",
    "    val_loss_dict[f'ann_hd_64_file_{n_files}_r'] = total_val_loss_list\n",
    "    test_loss_dict[f'ann_hd_64_file_{n_files}_r'] = total_test_loss_list\n",
    "torch.save(model, \"hd16r_adam_gm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45268f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
